{
    "model_size":1000,
    "architectures": [
      "MegatronForCausalLM"
    ],
    "attention_bias": false,
    "hidden_act": "swiglu",
    "hidden_size": 16384,
    "max_position_embeddings": 128000,
    "model_type": "llama",
    "num_attention_heads": 128,
    "num_hidden_layers": 126,
    "num_key_value_heads": 8,
    "torch_dtype": "bfloat16",
    "vocab_size": 128000
    }