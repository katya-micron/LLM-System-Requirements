{
    "model_size":1000,
    "architectures": [
      "MegatronForCausalLM"
    ],
    "attention_bias": false,
    "hidden_act": "relu",
    "hidden_size": 25600,
    "max_position_embeddings": 2048,
    "model_type": "megatron",
    "num_attention_heads": 128,
    "num_hidden_layers": 128,
    "num_key_value_heads": 128,
    "torch_dtype": "bfloat16",
    "vocab_size": 128256
    }